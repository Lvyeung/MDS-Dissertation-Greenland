{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import properscoring as ps\n",
    "from shapely.geometry import box\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Input\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Reshape, SpatialDropout2D, GlobalAveragePooling2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Assembling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GeoTIFF multiband spectral image\n",
    "\n",
    "file_path1 = \"Multispectral raster 1.tif\"\n",
    "file_path2 = \"Multispectral raster 2.tif\"\n",
    "cu_file_path = \"clean_geochem_df.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the bounds of the images\n",
    "\n",
    "with rasterio.open(file_path1) as dataset:\n",
    "    bounds1 = dataset.bounds\n",
    "    crs1 = dataset.crs\n",
    "    resolution1 = dataset.res\n",
    "    img = dataset.read(1)\n",
    "    \n",
    "with rasterio.open(file_path2) as dataset:\n",
    "    bounds2 = dataset.bounds\n",
    "    crs2 = dataset.crs\n",
    "    resolution2 = dataset.res\n",
    "\n",
    "print(\"Image bounds:\", bounds1)\n",
    "print(\"Coordinate reference system (CRS):\", crs1)\n",
    "print(\"Pixel resolution:\", resolution1)\n",
    "\n",
    "print(\"Image bounds:\", bounds2)\n",
    "print(\"Coordinate reference system (CRS):\", crs2)\n",
    "print(\"Pixel resolution:\", resolution2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function normalises pixel values for each band\n",
    "\n",
    "def band_normalisation(file_path):\n",
    "    \n",
    "    normalised_bands = []\n",
    "    \n",
    "#   Landsat-8 has data rescaled to digital numbers from 0 - 65536\n",
    "    for band in range(1,6):\n",
    "        # Open GeoTIFF file\n",
    "        with rasterio.open(file_path) as src:\n",
    "            L8_raster = src.read(band)\n",
    "\n",
    "        # Convert GeoTIFF to tensor\n",
    "        L8_raster = np.array(L8_raster)\n",
    "        L8_tensor = tf.convert_to_tensor(L8_raster, dtype=tf.float32)\n",
    "        \n",
    "        # Normalise all values in tensor\n",
    "        normalised_L8_tensor = L8_tensor / 65536\n",
    "        normalised_bands.append(normalised_L8_tensor)\n",
    "\n",
    "#   ASTER has data rescaled to digital numbers from 0 - 255\n",
    "    for band in range(6,9):\n",
    "        # Open GeoTIFF file\n",
    "        with rasterio.open(file_path) as src:\n",
    "            aster_raster = src.read(band)\n",
    "\n",
    "        # Convert GeoTIFF to tensor\n",
    "        aster_raster = np.array(aster_raster)\n",
    "        aster_tensor = tf.convert_to_tensor(aster_raster, dtype=tf.float32)\n",
    "        \n",
    "        # Normalise all values in tensor\n",
    "        normalised_aster_tensor = aster_tensor / 255\n",
    "        normalised_bands.append(normalised_aster_tensor)\n",
    "    \n",
    "#   Stacking normalised bands as tensors\n",
    "    stacked_tensor = np.stack(normalised_bands, axis=-1)\n",
    "    stacked_tensor = tf.expand_dims(stacked_tensor, axis=0)  # Add batch dimension for shape = (batch_size, height, width, bands)\n",
    "\n",
    "\n",
    "    return stacked_tensor          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_tensor_1 = band_normalisation(file_path1)\n",
    "print(normalised_tensor_1.shape)\n",
    "\n",
    "normalised_tensor_2 = band_normalisation(file_path2)\n",
    "print(normalised_tensor_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function extracts patches and removes any where all pixel values are 0\n",
    "\n",
    "def patchify(image_tensor, patch_size=(64, 64), stride=(64, 64)):\n",
    "    \n",
    "    # Extract patches using tf.image.extract_patches\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=image_tensor,\n",
    "        sizes=[1, patch_size[0], patch_size[1], 1],\n",
    "        strides=[1, stride[0], stride[1], 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding='VALID'\n",
    "    )\n",
    "    \n",
    "    # Get the dimensions of the input tensor\n",
    "    batch_size = tf.shape(image_tensor)[0]\n",
    "    patch_height, patch_width = patch_size\n",
    "    num_patches_h = tf.shape(patches)[1]  # Number of patches along height\n",
    "    num_patches_w = tf.shape(patches)[2]  # Number of patches along width\n",
    "    channels = tf.shape(image_tensor)[-1]  # Number of channels\n",
    "    \n",
    "    # Reshape patches to (num_patches, patch_height, patch_width, channels)\n",
    "    patches = tf.reshape(\n",
    "        patches, \n",
    "        [batch_size * num_patches_h * num_patches_w, patch_height, patch_width, channels]\n",
    "    )\n",
    "    \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# height + width split into 46 = 46 x 46 = 2116 patches per image\n",
    "patches_trainset = patchify(normalised_tensor_1)\n",
    "print(patches_trainset.shape)\n",
    "\n",
    "patches_valtestset = patchify(normalised_tensor_2)\n",
    "print(patches_valtestset.shape)\n",
    "\n",
    "# with open(\"patches_trainset.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(patches_trainset, file)\n",
    "    \n",
    "# with open(\"patches_valtestset.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(patches_valtestset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"patches_trainset.pkl\", \"rb\") as file:\n",
    "    patches_trainset = pickle.load(file)\n",
    "\n",
    "with open(\"patches_valtestset.pkl\", \"rb\") as file:\n",
    "    patches_valtestset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping copper points to patches\n",
    "\n",
    "# Image metadata\n",
    "image_bounds = {\n",
    "    'left': 479295.0,\n",
    "    'bottom': 8713575.0,\n",
    "    'right': 568305.0,\n",
    "    'top': 8802045.0\n",
    "}\n",
    "pixel_resolution = 30.0  # in meters\n",
    "patch_size = 64  # in pixels\n",
    "num_patches_x = 46  # number of patches along x-direction\n",
    "num_patches_y = 46  # number of patches along y-direction\n",
    "patch_width = 1920\n",
    "patch_height = 1920\n",
    "\n",
    "# Generate patch bounding boxes in UTM coordinates\n",
    "patch_bboxes = []\n",
    "for i in range(num_patches_x):\n",
    "    for j in range(num_patches_y):\n",
    "        # Calculate lower-left corner (x, y) of the patch\n",
    "        x = image_bounds['left'] + i * patch_width\n",
    "        y = image_bounds['bottom'] + j * patch_height\n",
    "        \n",
    "        # Create the bounding box using the lower-left and upper-right corners\n",
    "        patch_bbox = box(x, y, x + patch_width, y + patch_height)\n",
    "        patch_bboxes.append(patch_bbox)\n",
    "\n",
    "patches_gdf = gpd.GeoDataFrame(geometry=patch_bboxes, crs=\"EPSG:32619\")\n",
    "patches_gdf = patches_gdf.rename_axis('patch_id')\n",
    "\n",
    "patches_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load copper points from df and convert to gdf with correct CRS\n",
    "with open(cu_file_path, \"rb\") as file:\n",
    "    clean_geochem_df = pickle.load(file)\n",
    "    \n",
    "cu_samples_df = clean_geochem_df[[\"Cu\", \"Longitude\", \"Latitude\"]]\n",
    "cu_samples_df = cu_samples_df.copy()\n",
    "cu_samples_df.loc[:, 'geometry'] = cu_samples_df.apply(lambda row: Point(row['Longitude'], row['Latitude']), axis=1)\n",
    "\n",
    "cu_points_df = cu_samples_df[cu_samples_df[\"Cu\"] != 0.0].reset_index()\n",
    "\n",
    "cu_points_gdf = gpd.GeoDataFrame(cu_points_df, geometry=cu_points_df[\"geometry\"], crs=\"EPSG:4326\")\n",
    "cu_points_gdf = cu_points_gdf.to_crs(epsg=32619)\n",
    "\n",
    "cu_points_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform the spatial join to check if points fall within any patch\n",
    "labeled_patches_gdf = gpd.sjoin(patches_gdf, cu_points_gdf,  how=\"left\", predicate=\"intersects\")\n",
    "labeled_patches_gdf = labeled_patches_gdf[['Cu','geometry']]\n",
    "\n",
    "# Convert all NaN values to 0\n",
    "labeled_patches_gdf = labeled_patches_gdf.fillna(0)\n",
    "\n",
    "# Aggregate Cu values for points in the same patch\n",
    "labeled_patches_gdf = labeled_patches_gdf.groupby(labeled_patches_gdf.index).agg({\"Cu\": \"mean\", \"geometry\": \"first\"})\n",
    "y_labels = labeled_patches_gdf.iloc[:, 0]\n",
    "copper_present_gdf = labeled_patches_gdf.iloc[labeled_patches_gdf[\"Cu\"].to_numpy().nonzero()[0]]\n",
    "\n",
    "# Showing only patches with a non-zero Cu value\n",
    "print(\"Shape of y variable:\", y_labels.shape)\n",
    "print(\"Patches with non-zero Cu value:\")\n",
    "display(copper_present_gdf)\n",
    "display(labeled_patches_gdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.histplot(data=copper_present_gdf, x='Cu', bins=30)\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.xlabel('Observed Volume of Copper (ppm)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Train/test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating indexes for train set indexing\n",
    "\n",
    "random.seed(35)\n",
    "train_index = random.sample(range(2116), 1482)\n",
    "train_index_tf = tf.convert_to_tensor(train_index, dtype=tf.int32)\n",
    "\n",
    "# Creating indexes for test and validation set indexing whilst ensuring there is no overlap between any sets\n",
    "valtest_index = list(set(random.sample(range(2116), 2116)) - set(train_index)) # Finding indices not in train_index\n",
    "shuffled = np.random.permutation(valtest_index) # Shuffling indices remaining in valtest_index\n",
    "split_index = len(valtest_index) // 2 # Index to split at\n",
    "\n",
    "test_index = shuffled[:split_index]\n",
    "test_index_tf = tf.convert_to_tensor(test_index, dtype=tf.int32)\n",
    "\n",
    "val_index = shuffled[split_index:]\n",
    "val_index_tf = tf.convert_to_tensor(test_index, dtype=tf.int32)\n",
    "\n",
    "# Index the TensorFlow datasets\n",
    "X_train = tf.gather(patches_trainset, train_index_tf)\n",
    "y_train = tf.gather(y_labels, train_index_tf)\n",
    "\n",
    "X_test = tf.gather(patches_valtestset, test_index_tf)\n",
    "y_test = tf.gather(y_labels, test_index_tf)\n",
    "\n",
    "X_val = tf.gather(patches_valtestset, val_index_tf)\n",
    "y_val = tf.gather(y_labels, val_index_tf)\n",
    "\n",
    "print(f\"Number of patches in training set: {len(train_index)}\")\n",
    "print(f\"Number of patches in testing set: {len(test_index)}\")\n",
    "print(f\"Number of patches in validation set: {len(val_index)}\")\n",
    "\n",
    "print(f\"\\nShape of patches in training set: {X_train.shape}\")\n",
    "print(f\"Shape of labels in training set: {y_train.shape}\")\n",
    "\n",
    "print(f\"\\nShape of patches in testing set: {X_test.shape}\")\n",
    "print(f\"Shape of labels in testing set: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nShape of patches in validation set: {X_val.shape}\")\n",
    "print(f\"Shape of patches in validation set: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) BCNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "# Custom loss function for negative log likelihood\n",
    "\n",
    "@register_keras_serializable() # Allow for custom function to be recognised during unpickling training history\n",
    "\n",
    "def NLL_loss_function(y, model_output):\n",
    "    return -tf.reduce_mean(model_output.log_prob(y))\n",
    "\n",
    "def normal_distribution(x):\n",
    "    mean = x[..., :1]  # Extracting mean\n",
    "    log_variance = x[..., 1:]  # Extracting log-variance\n",
    "    variance = tf.math.softplus(log_variance)  # softplus to ensure variance is positive\n",
    "    return tfd.Normal(loc=mean, scale=variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Model Architecture\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "BCNN = Sequential(name=\"BayesianCNN\")\n",
    "\n",
    "# Input layer\n",
    "BCNN.add(Input(shape=(64, 64, 8)))  # Shape of each patch\n",
    "\n",
    "# First convolutional layer with pooling\n",
    "BCNN.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "BCNN.add(SpatialDropout2D(rate=0.3))\n",
    "BCNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Second convolutional layer with pooling\n",
    "BCNN.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "BCNN.add(SpatialDropout2D(rate=0.3))\n",
    "BCNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Third convolutional layer\n",
    "BCNN.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "BCNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening\n",
    "BCNN.add(GlobalAveragePooling2D())\n",
    "\n",
    "# Fully connected dense layer\n",
    "BCNN.add(Dense(128, activation='relu'))\n",
    "BCNN.add(Dropout(0.3))\n",
    "\n",
    "# Output layer (Mean and Log Variance)\n",
    "BCNN.add(Dense(2))\n",
    "\n",
    "# Convert to a probabilistic output\n",
    "BCNN.add(tfp.layers.DistributionLambda(normal_distribution))# Using softplus link function to ensure variance is positive\n",
    "\n",
    "\n",
    "# Model summary\n",
    "BCNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Compile the model\n",
    "BCNN.compile(loss=NLL_loss_function,\n",
    "             optimizer=opt,\n",
    "             metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with early stop\n",
    "early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, mode=\"auto\")\n",
    "\n",
    "BCNNhistory_earlystop = BCNN.fit(X_train, y_train, \n",
    "                        epochs=EPOCHS, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        validation_split=0.1,\n",
    "                        callbacks=[early],\n",
    "                        validation_data=(X_val, y_val)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting training history\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "# Plotting accuracy and validated accuracy\n",
    "ax1.set_title('Figure 3. BCNN Training and Validated MSE and Loss')\n",
    "ax1.plot(BCNNhistory_earlystop.history['mse'], label='Training MSE', color='green')\n",
    "ax1.plot(BCNNhistory_earlystop.history['val_mse'], label='Validation MSE', color='pink')\n",
    "ax1.set_ylim(bottom=0.0)\n",
    "ax1.set_ylabel('MSE')\n",
    "ax1.legend()\n",
    "\n",
    "# Plotting loss and validated loss\n",
    "ax2.plot(BCNNhistory_earlystop.history['loss'], label='Training Loss', color='purple')\n",
    "ax2.plot(BCNNhistory_earlystop.history['val_loss'], label='Validation Loss', color='red')\n",
    "ax2.set_ylim(bottom=0.0)\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training history\n",
    "trainhist = pd.DataFrame({'training': BCNNhistory_earlystop.history['loss'], 'testing': BCNNhistory_earlystop.history['val_loss']})\n",
    "trainhist['epoch'] = np.arange(1, len(BCNNhistory_earlystop.history['loss'])+1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(data=pd.melt(trainhist, id_vars='epoch', value_name='NLL', var_name='dataset'), x='epoch', y='NLL', hue='dataset')\n",
    "plt.ylim(0, np.quantile(trainhist['testing'], 0.999))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Negative Log Likelihood')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Prediction without Monte Carlo Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict mean and log variance\n",
    "mean_preds = BCNN(X_test).mean()\n",
    "variance_preds = BCNN(X_test).variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create dataframe of observed and predicted test values using model\n",
    "preds_df = pd.DataFrame({\n",
    "    'Observations': y_test.numpy().ravel(),\n",
    "    'Mean Predictions': mean_preds.numpy().ravel(),\n",
    "    'Variance Predictions': variance_preds.numpy().ravel()\n",
    "})\n",
    "\n",
    "# Calculate R Squared and RMSE\n",
    "r_squared = np.round(np.corrcoef(preds_df['Mean Predictions'], preds_df['Observations'])[0, 1] ** 2, 3)\n",
    "rmse = np.round(np.sqrt(np.mean((preds_df['Mean Predictions'] - preds_df['Observations']) ** 2)), 3)\n",
    "\n",
    "# Calculate CRPS\n",
    "preds_df[\"Standard Dev\"] = np.sqrt(preds_df[\"Variance Predictions\"])\n",
    "preds_df[\"CRPS\"] = preds_df.apply(lambda row: ps.crps_gaussian(row['Observations'], row['Mean Predictions'], row['Standard Dev']), axis=1)\n",
    "mean_crps = preds_df[\"CRPS\"].mean()\n",
    "\n",
    "# Evaluation metrics summary\n",
    "eval_df = pd.DataFrame({\n",
    "    \"Evaluation Metric\": [\"R-squared\", \"RMSE\", \"Mean CRPS\"],\n",
    "    \"Metric Score\": [r_squared, rmse, mean_crps]\n",
    "})\n",
    "\n",
    "display(preds_df)\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding average aleatoric uncertainty\n",
    "\n",
    "alea_unc = preds_df[\"Variance Predictions\"].mean()\n",
    "alea_unc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting uncertainty within predictions in test set\n",
    "\n",
    "x = np.arange(317)  # Indices of predictions\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(x, \n",
    "             np.squeeze(mean_preds), \n",
    "             yerr=np.sqrt(np.squeeze(variance_preds)), \n",
    "             fmt='o', \n",
    "             color='black',\n",
    "             ecolor='blue',\n",
    "             elinewidth=0.5,\n",
    "             label='Predictions with Uncertainty')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title('Predictions with Uncertainty: Without Monte Carlo Dropout')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Prediction with Monte Carlo Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict(model, X, num_samples):\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        predictions.append(model(X, training=True).mean())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    mean_predictions = predictions.mean(axis=0)\n",
    "    uncertainty = predictions.var(axis=0)\n",
    "\n",
    "    return mean_predictions, uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MC Dropout\n",
    "mean_predictions, uncertainty = mc_dropout_predict(BCNN, X_test, num_samples=50)\n",
    "\n",
    "\n",
    "# Create dataframe of observed and predicted test values using model\n",
    "mc_preds_df = pd.DataFrame({\n",
    "    'Observations': y_test.numpy().ravel(),\n",
    "    'Mean Predictions': mean_predictions.ravel(),\n",
    "    'Variance Predictions': uncertainty.ravel()\n",
    "})\n",
    "\n",
    "# Calculate R Squared and RMSE\n",
    "mc_r_squared = np.round(np.corrcoef(mc_preds_df['Mean Predictions'], mc_preds_df['Observations'])[0, 1] ** 2, 3)\n",
    "mc_rmse = np.round(np.sqrt(np.mean((mc_preds_df['Mean Predictions'] - mc_preds_df['Observations']) ** 2)), 3)\n",
    "\n",
    "# Calculate CRPS\n",
    "mc_preds_df[\"Standard Dev\"] = np.sqrt(mc_preds_df[\"Variance Predictions\"])\n",
    "mc_preds_df[\"CRPS\"] = mc_preds_df.apply(lambda row: ps.crps_gaussian(row['Observations'], row['Mean Predictions'], row['Standard Dev']), axis=1)\n",
    "mc_mean_crps = mc_preds_df[\"CRPS\"].mean()\n",
    "\n",
    "# Evaluation metrics summary\n",
    "mc_eval_df = pd.DataFrame({\n",
    "    \"Evaluation Metric\": [\"R-squared\", \"RMSE\", \"Mean CRPS\"],\n",
    "    \"Metric Score\": [mc_r_squared, mc_rmse, mc_mean_crps]\n",
    "})\n",
    "\n",
    "display(mc_preds_df)\n",
    "display(mc_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting uncertainty within predictions in test set\n",
    "\n",
    "x = np.arange(317)  # Indices of predictions\n",
    "mean_predictions = np.squeeze(mean_predictions)  # Remove singleton dimension\n",
    "uncertainty = np.squeeze(uncertainty)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(x, \n",
    "             mean_predictions, \n",
    "             yerr=np.sqrt(uncertainty), \n",
    "             fmt='o', \n",
    "             color='black',\n",
    "             ecolor='blue',\n",
    "             elinewidth=0.5,\n",
    "             label='Predictions with Uncertainty')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title('Predictions with Uncertainty: With Monte Carlo Dropout')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding average epistemic uncertainty\n",
    "\n",
    "epi_unc = mc_preds_df[\"Variance Predictions\"].mean()\n",
    "epi_unc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total uncertainty\n",
    "\n",
    "total_unc = alea_unc + epi_unc\n",
    "total_unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Apply The Model to the Area of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating batches\n",
    "all_observations = labeled_patches_gdf.iloc[:, 0]\n",
    "\n",
    "X_batches = tf.split(patches_trainset, num_or_size_splits=4, axis=0)\n",
    "y_batches = np.array_split(all_observations, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC Dropout for entire region\n",
    "ig_mean_predictions, ig_uncertainty = mc_dropout_predict(BCNN, batches[0], num_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC Dropout for entire region\n",
    "\n",
    "batch_dfs = []\n",
    "\n",
    "for i in range(len(X_batches)):\n",
    "    batch_mean_predictions, batch_uncertainty = mc_dropout_predict(BCNN, batches[i], num_samples=50)\n",
    "    \n",
    "    batch_mc_preds_df = pd.DataFrame({\n",
    "        'Observations': y_batches[i],\n",
    "        'Mean Predictions': batch_mean_predictions.ravel(),\n",
    "        'Variance Predictions': batch_uncertainty.ravel()\n",
    "    })\n",
    "    \n",
    "    batch_dfs.append(batch_mc_preds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inglefield_mc_preds_df = pd.concat(batch_dfs, axis=0, ignore_index=True) # Concatenating batches\n",
    "inglefield_mc_preds_df = pd.concat([inglefield_mc_preds_df, labeled_patches_gdf], axis=1)\n",
    "\n",
    "# Saving predictions\n",
    "\n",
    "# with open(\"inglefield_mc_preds_df.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(inglefield_mc_preds_df, file)\n",
    "\n",
    "with open(\"inglefield_mc_preds_df.pkl\", \"rb\") as file:\n",
    "    inglefield_mc_preds_df = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dataframe of model predictions, mapping to polygon geometry for plotting\n",
    "\n",
    "inglefield_mc_preds_gdf = gpd.GeoDataFrame(inglefield_mc_preds_df, geometry='geometry')\n",
    "inglefield_mc_preds_gdf = inglefield_mc_preds_gdf.set_crs('EPSG:32619')\n",
    "\n",
    "inglefield_mc_preds_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting prediction data and overlaying on raster image\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))  # Customize the size as needed\n",
    "\n",
    "with rasterio.open(file_path1) as dataset:\n",
    "    img = dataset.read(1)\n",
    "\n",
    "    show(dataset, ax=ax, title='Raster Image', cmap='gray')\n",
    "\n",
    "    # Check if CRS matches\n",
    "    if inglefield_mc_preds_gdf.crs != crs1:\n",
    "        inglefield_mc_preds_gdf = inglefield_mc_preds_gdf.to_crs(crs1)\n",
    "\n",
    "    # Plot polygons with transparency\n",
    "    inglefield_mc_preds_gdf.plot(ax=ax, \n",
    "                                 column='Mean Predictions', \n",
    "                                 cmap='viridis', \n",
    "                                 legend=True, \n",
    "                                 alpha=0.5, \n",
    "                                 edgecolor='grey',\n",
    "                                 linewidth=0.25)\n",
    "\n",
    "    ax.set_xlim(bounds1[0], bounds1[2])  # xmin, xmax\n",
    "    ax.set_ylim(bounds1[1], bounds1[3])  # ymin, ymax\n",
    "\n",
    "    ax.set_title('Monte Carlo Dropout Predicted Cu (ppm)')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(\"MC preds overlain.svg\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis_kernel",
   "language": "python",
   "name": "gis_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
